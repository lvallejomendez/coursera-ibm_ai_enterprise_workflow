{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"m1-u6-case-study.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DTs3HdwAEEKo"},"source":["## Setup\n","\n","```python\n","# Setup\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/Colab Notebooks/..directory/')\n","os.getcwd()\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qsPu-HP0bBwZ","executionInfo":{"status":"ok","timestamp":1619817302993,"user_tz":-120,"elapsed":548,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"03de736c-3674-4891-c43d-44b67a54bbee"},"source":["# Setup\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lbiwr-dqbWL7","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1619817303318,"user_tz":-120,"elapsed":862,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"7119a49c-cbea-4326-e39a-1bb87f62ed5f"},"source":["#@title working dir {display-mode: \"form\"}\n","# This code will be hidden when the notebook is loaded.\n","os.chdir('/content/drive/My Drive/Colab Notebooks/Coursera/IBM/IBM AI EWS/Case-Study/notebook/')\n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks/Coursera/IBM/IBM AI EWS/Case-Study/notebook'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"9t2lQ2wfIdI7"},"source":["# Case Study - Data ingestion"]},{"cell_type":"markdown","metadata":{"id":"Nz6Ul5sHIdJF"},"source":["The goal of this case study is to put into practice the important concepts from module 1.  We will go through the basic process that begins with refining the business opportunity and ensuring that it is articulated using a scientific thought process.\n","\n","The business opportunity and case study was first mentioned in Unit 2 of module 1 and like the AAVIAL company itself these data were created for learning purposes.  We will be using the AAVAIL example as a basis for this case study. You will be gathering data from several provided sources, staging it for quality assurance and saving it in a target destination that is most appropriate.\n","\n","Watch the video to review the important concepts from the units you just covered and to see an overview of the objectives for this case study."]},{"cell_type":"markdown","metadata":{"id":"_nmyYUCNIdJI"},"source":["## Case study overall objectives\n","\n","1. Gather all relevant data from the sources of provided data\n","2. Implement several checks for quality assurance \n","3. Take the initial steps towards automation of the ingestion pipeline\n","\n","## Getting started\n","\n","Download this notebook and open it locally using a Jupyter server. Alternatively you may use Watson Studio.  To make using Watson Studio easier we have provided a zip archive file containing the files needed to complete this case study in Watson Studio. See the [Getting started with Watson Studio](m1-u5-5-watson-studio.rst) page.\n","\n","**You will need the following files to complete this case study**\n","\n","* [m1-u6-case-study.ipynb](m1-u6-case-study.ipynb)\n","* [m1-u6-case-study-solution.ipynb](./notebooks/m1-u6-case-study-solution.ipynb)\n","* [aavail-customers.db](./data/aavail-customers.db)\n","* [aavail-steams.csv](./data/aavail-streams.csv)\n","\n","1. Fill in all of the places in this notebook marked with ***YOUR CODE HERE*** or ***YOUR ANSWER HERE***\n","2. When you have finished the case study there will be a short quiz\n","\n","You may review the rest of this content as part of the notebook, but once you are ready to get started be ensure that you are working with a *live* version either as part of Watson Studio or locally."]},{"cell_type":"markdown","metadata":{"id":"XvUI6YhCIdJJ"},"source":["## Data Sources\n","\n","The data you will be sourcing from is contained in two sources.\n","\n","1. A database ([SQLite](https://www.sqlite.org/index.html)) of `customer` data\n","2. A [CSV file](https://en.wikipedia.org/wiki/Comma-separated_values) of `stream` level data\n","\n","   >You will create a simple data pipeline that\n","   (1) simplifies the data for future analysis\n","   (2) performs quality assurance checks.\n","\n","The process of building *the data ingestion pipeline* entails extracting data, transforming it, and loading it into an appropriate data storage technology.  When constructing a pipeline it is important to keep in mind that they generally process data in batches.  For example, data may be compiled during the day and the batch could be processed during the night.  The data pipeline may also be optimized to execute as a streaming computation (i.e., every event is handled as it occurs)."]},{"cell_type":"markdown","metadata":{"id":"LxSkdTHRIdJK"},"source":["## PART 1: Gathering the data\n","\n","The following is an [Entity Relationship Diagram (ERD)](https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model) that details the tables and contents of the database.\n","\n","![aavail-schema.png](attachment:aavail-schema.png)"]},{"cell_type":"code","metadata":{"id":"dtujlT85IdJL","executionInfo":{"status":"ok","timestamp":1619817303319,"user_tz":-120,"elapsed":857,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}}},"source":["## all the imports needed for this case study\n","import os\n","import pandas as pd\n","import numpy as np\n","import sqlite3\n","\n","## specify the directory you saved the data in\n","data_dir = os.path.join(\"..\",\"data\")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBZgC201IdJM"},"source":["Much of the data exist in a database.  You can connect to it using the `sqlite3` Python package with the function shown below.  Note that is is good practice to wrap your connect functions in a [try-except statement](https://docs.python.org/3/tutorial/errors.html) to cleanly handle exceptions."]},{"cell_type":"code","metadata":{"id":"2aYUUMZ7IdJM","executionInfo":{"status":"ok","timestamp":1619817303320,"user_tz":-120,"elapsed":854,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}}},"source":["def connect_db(file_path):\n","    try:\n","        conn = sqlite3.connect(file_path)\n","        print(\"...successfully connected to db\\n\")\n","    except Error as e:\n","        print(\"...unsuccessful connection\\n\",e)\n","    \n","    return(conn)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLcBPn4kIdJN","executionInfo":{"status":"ok","timestamp":1619817303321,"user_tz":-120,"elapsed":852,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"9904c3d3-2bd1-4b93-9ee2-b6e36ffbfa73"},"source":["## make the connection to the database\n","conn = connect_db(os.path.join(data_dir,\"aavail-customers.db\"))\n","\n","## print the table names\n","tables = [t[0] for t in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")]\n","print(tables)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["...successfully connected to db\n","\n","['CUSTOMER', 'INVOICE', 'INVOICE_ITEM', 'COUNTRY']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tqscOA73IdJP"},"source":["### QUESTION 1:\n","\n","**extract the relevant data from the DB**\n","\n","Query the database and extract the following data into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n"," \n","* Customer ID (integer)\n","* Last name\n","* First name\n","* DOB (Date Of Birth)\n","* City\n","* State\n","* Country (the name NOT the country_id)\n","* Gender\n","\n","Remember that that SQL is case-insensitive, but it is traditional to use ALL CAPS for SQL keywords. It is also a convention to end SQL statements with a semi-colon.  \n","\n","#### Resources\n","\n","* [W3 schools SQL tutorial](https://www.w3schools.com/sql)\n","* [W3 schools SQL joins](https://www.w3schools.com/sql/sql_join.asp)"]},{"cell_type":"markdown","metadata":{"id":"QcrJJeoYK2Ek"},"source":["```python\n","# first query\n","\n","for c in conn.execute('SELECT customer_id, last_name, first_name, DOB, city, state, gender FROM CUSTOMER'):\n","    print(c)\n","```"]},{"cell_type":"markdown","metadata":{"id":"cOwHhnM-K1ze"},"source":["```python\n","# joining the tables\n","\n","for c in conn.execute('SELECT customer_id, last_name, first_name, DOB, city, state, country_name, gender \\\n","FROM CUSTOMER \\\n","INNER JOIN COUNTRY ON CUSTOMER.country_id = COUNTRY.country_id'):\n","    print(c)\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"ELy0BUDMIdJQ","executionInfo":{"status":"ok","timestamp":1619817303639,"user_tz":-120,"elapsed":1159,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"0fe7782b-4b89-420e-abb1-0cf6b1dbb6f4"},"source":["## YOUR CODE HERE\n","\n","# the query to DataFrame\n","\n","customer_df = conn.execute('SELECT customer_id, last_name, first_name, DOB, city, state, country_name, gender \\\n","FROM CUSTOMER \\\n","INNER JOIN COUNTRY ON CUSTOMER.country_id = COUNTRY.country_id;')\n","\n","columns_name = ['customer_id', 'last_name', 'first_name', 'DOB', 'city', 'state', 'country_name', 'gender']\n","\n","customer_df = pd.DataFrame(customer_df, columns=columns_name)\n","customer_df.head(15)\n"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>last_name</th>\n","      <th>first_name</th>\n","      <th>DOB</th>\n","      <th>city</th>\n","      <th>state</th>\n","      <th>country_name</th>\n","      <th>gender</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Todd</td>\n","      <td>Kasen</td>\n","      <td>07/30/98</td>\n","      <td>Rock Hill</td>\n","      <td>South Carolina</td>\n","      <td>united_states</td>\n","      <td>m</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Garza</td>\n","      <td>Ensley</td>\n","      <td>04/12/89</td>\n","      <td>singapore</td>\n","      <td>None</td>\n","      <td>singapore</td>\n","      <td>f</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Carey</td>\n","      <td>Lillian</td>\n","      <td>09/12/97</td>\n","      <td>Auburn</td>\n","      <td>Alabama</td>\n","      <td>united_states</td>\n","      <td>f</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Christensen</td>\n","      <td>Beau</td>\n","      <td>01/28/99</td>\n","      <td>Hempstead</td>\n","      <td>New York</td>\n","      <td>united_states</td>\n","      <td>m</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Gibson</td>\n","      <td>Ernesto</td>\n","      <td>03/23/98</td>\n","      <td>singapore</td>\n","      <td>None</td>\n","      <td>singapore</td>\n","      <td>m</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>Murray</td>\n","      <td>Deshawn</td>\n","      <td>09/18/97</td>\n","      <td>Portland</td>\n","      <td>Maine</td>\n","      <td>united_states</td>\n","      <td>m</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>Tate</td>\n","      <td>Daxton</td>\n","      <td>12/23/70</td>\n","      <td>singapore</td>\n","      <td>None</td>\n","      <td>singapore</td>\n","      <td>m</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>Small</td>\n","      <td>Tenley</td>\n","      <td>07/21/72</td>\n","      <td>Paterson</td>\n","      <td>New Jersey</td>\n","      <td>united_states</td>\n","      <td>f</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>Chase</td>\n","      <td>Kyra</td>\n","      <td>05/19/98</td>\n","      <td>Temple</td>\n","      <td>Texas</td>\n","      <td>united_states</td>\n","      <td>f</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>Barber</td>\n","      <td>London</td>\n","      <td>07/17/93</td>\n","      <td>Somerville</td>\n","      <td>Massachusetts</td>\n","      <td>united_states</td>\n","      <td>f</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>Rivas</td>\n","      <td>Rohan</td>\n","      <td>01/23/05</td>\n","      <td>Pawtucket</td>\n","      <td>Rhode Island</td>\n","      <td>united_states</td>\n","      <td>m</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>Ross</td>\n","      <td>Romina</td>\n","      <td>05/05/80</td>\n","      <td>singapore</td>\n","      <td>None</td>\n","      <td>singapore</td>\n","      <td>f</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>Moore</td>\n","      <td>Liberty</td>\n","      <td>03/04/99</td>\n","      <td>Bristol</td>\n","      <td>Connecticut</td>\n","      <td>united_states</td>\n","      <td>f</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>Huber</td>\n","      <td>Yusuf</td>\n","      <td>07/22/00</td>\n","      <td>Woburn</td>\n","      <td>Massachusetts</td>\n","      <td>united_states</td>\n","      <td>m</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>Suarez</td>\n","      <td>Brantley</td>\n","      <td>11/04/96</td>\n","      <td>Decatur</td>\n","      <td>Illinois</td>\n","      <td>united_states</td>\n","      <td>m</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    customer_id    last_name first_name  ...           state   country_name gender\n","0             1         Todd      Kasen  ...  South Carolina  united_states      m\n","1             2        Garza     Ensley  ...            None      singapore      f\n","2             3        Carey    Lillian  ...         Alabama  united_states      f\n","3             4  Christensen       Beau  ...        New York  united_states      m\n","4             5       Gibson    Ernesto  ...            None      singapore      m\n","5             6       Murray    Deshawn  ...           Maine  united_states      m\n","6             7         Tate     Daxton  ...            None      singapore      m\n","7             8        Small     Tenley  ...      New Jersey  united_states      f\n","8             9        Chase       Kyra  ...           Texas  united_states      f\n","9            10       Barber     London  ...   Massachusetts  united_states      f\n","10           11        Rivas      Rohan  ...    Rhode Island  united_states      m\n","11           12         Ross     Romina  ...            None      singapore      f\n","12           13        Moore    Liberty  ...     Connecticut  united_states      f\n","13           14        Huber      Yusuf  ...   Massachusetts  united_states      m\n","14           15       Suarez   Brantley  ...        Illinois  united_states      m\n","\n","[15 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"MYmQsJFRIdJR"},"source":["### QUESTION 2:\n","\n","**Extract the relevant data from the CSV file**\n","\n","For each ```customer_id``` determine if a customer has stopped their subscription or not and save it in a dictionary or another data container."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"mSDdA_WLIdJS","executionInfo":{"status":"ok","timestamp":1619817303641,"user_tz":-120,"elapsed":1152,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"0eb8c64a-6b3f-46b1-e78a-de18fa070c2b"},"source":["df_streams = pd.read_csv(os.path.join(data_dir,r\"aavail-streams.csv\"))\n","df_streams.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>stream_id</th>\n","      <th>date</th>\n","      <th>invoice_item_id</th>\n","      <th>subscription_stopped</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1420.0</td>\n","      <td>2018-10-21</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1343.0</td>\n","      <td>2018-10-23</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1756.0</td>\n","      <td>2018-11-05</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1250.0</td>\n","      <td>2018-11-06</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1324.0</td>\n","      <td>2018-11-12</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   customer_id  stream_id        date  invoice_item_id  subscription_stopped\n","0            1     1420.0  2018-10-21              2.0                     0\n","1            1     1343.0  2018-10-23              2.0                     0\n","2            1     1756.0  2018-11-05              2.0                     0\n","3            1     1250.0  2018-11-06              2.0                     0\n","4            1     1324.0  2018-11-12              2.0                     0"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7C1MF7WRIdJS","executionInfo":{"status":"ok","timestamp":1619817303642,"user_tz":-120,"elapsed":1149,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"ba6e41fa-e654-41ae-e944-068056ce578a"},"source":["## YOUR CODE HERE\n","# DataFrame info.\n","df_streams.info()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 18859 entries, 0 to 18858\n","Data columns (total 5 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   customer_id           18859 non-null  int64  \n"," 1   stream_id             17695 non-null  float64\n"," 2   date                  18859 non-null  object \n"," 3   invoice_item_id       18859 non-null  float64\n"," 4   subscription_stopped  18859 non-null  int64  \n","dtypes: float64(2), int64(2), object(1)\n","memory usage: 736.8+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hxMbfZoGLik0","executionInfo":{"status":"ok","timestamp":1619817303643,"user_tz":-120,"elapsed":1145,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"05b08e6a-8394-4a29-e654-185753494e7c"},"source":["# \"customer_id\" unique values\n","df_streams.customer_id.nunique()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8BsLG_bLjMd","executionInfo":{"status":"ok","timestamp":1619817303645,"user_tz":-120,"elapsed":1143,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"d74c4c76-7aaf-4c5d-cde3-b194970e5f3c"},"source":["# \"subscription_stopped\" values. 0 for not, 1 for yes.\n","df_streams.subscription_stopped.value_counts()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    18570\n","1      289\n","Name: subscription_stopped, dtype: int64"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"EBHGyhWGLpwM","executionInfo":{"status":"ok","timestamp":1619817305915,"user_tz":-120,"elapsed":3404,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"4deb0c3d-0d36-4443-92ce-8db8540e158f"},"source":["# grouping for \"customer_id\" and looking for the max value per grouped id.\n","df_streams.groupby(\"customer_id\").subscription_stopped.describe()"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>min</th>\n","      <th>25%</th>\n","      <th>50%</th>\n","      <th>75%</th>\n","      <th>max</th>\n","    </tr>\n","    <tr>\n","      <th>customer_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>24.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13.0</td>\n","      <td>0.076923</td>\n","      <td>0.277350</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>22.0</td>\n","      <td>0.045455</td>\n","      <td>0.213201</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>19.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>24.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>15.0</td>\n","      <td>0.066667</td>\n","      <td>0.258199</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>24.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>20.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>16.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1000</th>\n","      <td>21.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 8 columns</p>\n","</div>"],"text/plain":["             count      mean       std  min  25%  50%  75%  max\n","customer_id                                                    \n","1             24.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n","2             13.0  0.076923  0.277350  0.0  0.0  0.0  0.0  1.0\n","3             22.0  0.045455  0.213201  0.0  0.0  0.0  0.0  1.0\n","4             19.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n","5             24.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n","...            ...       ...       ...  ...  ...  ...  ...  ...\n","996           15.0  0.066667  0.258199  0.0  0.0  0.0  0.0  1.0\n","997           24.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n","998           20.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n","999           16.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n","1000          21.0  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0\n","\n","[1000 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nrN3LS_wLzt8","executionInfo":{"status":"ok","timestamp":1619817305916,"user_tz":-120,"elapsed":3400,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"e2ef5f95-c2c7-4730-c6e4-f38b8c2f04bb"},"source":["# For each customer_id determine if a customer has stopped their subscription or not and save it in a data container.\n","has_stopped = df_streams.groupby(\"customer_id\").apply(lambda x : True if x['subscription_stopped'].max() > 0 else False)\n","has_stopped"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["customer_id\n","1       False\n","2        True\n","3        True\n","4       False\n","5       False\n","        ...  \n","996      True\n","997     False\n","998     False\n","999     False\n","1000    False\n","Length: 1000, dtype: bool"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"QdMIXF9zIdJS"},"source":["## PART 2: Checks for quality assurance\n","\n","Sometimes it is known in advance which types of data integrity issues to expect, but other times it is during the Exploratory Data Analysis (EDA) process that these issues are identified.  After extracting data it is important to include checks for quality assurance even on the first pass through the AI workflow.  Here you will combine the data into a single structure and provide a couple checks for quality assurance.\n","\n","### QUESTION 3: \n","\n","**Implement checks for quality assurance**\n","\n","1. In the customer dataframe loaded question 1, remove any repeat customers based on ```customer_id```\n","2. In the streams dataset, remove stream data that do not have an associated ```stream_id```\n","3. Check for missing values in both datasets."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_VYp185IdJT","executionInfo":{"status":"ok","timestamp":1619817305916,"user_tz":-120,"elapsed":3397,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"a9e8d3dc-ae67-4a87-c7ef-6ebd84903af9"},"source":["## YOUR CODE HERE\n","# \"customer_id\" repeated values in customer DF\n","customer_df.customer_id.value_counts()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1      2\n","11     2\n","21     2\n","31     2\n","401    2\n","      ..\n","661    1\n","660    1\n","659    1\n","658    1\n","500    1\n","Name: customer_id, Length: 1000, dtype: int64"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"Y4m9ELQLMK5M","executionInfo":{"status":"ok","timestamp":1619817305916,"user_tz":-120,"elapsed":3390,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}}},"source":["# Dropping duplicates\n","customer_df.drop_duplicates(inplace=True)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EIINP48uML7k","executionInfo":{"status":"ok","timestamp":1619817305917,"user_tz":-120,"elapsed":3388,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"e3db97c6-db1e-4123-9e26-87d5b6332cc3"},"source":["# new \"customer_id\" values in customer DF\n","customer_df.customer_id.value_counts()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000    1\n","329     1\n","342     1\n","341     1\n","340     1\n","       ..\n","662     1\n","661     1\n","660     1\n","659     1\n","1       1\n","Name: customer_id, Length: 1000, dtype: int64"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7HqwJySBMMUM","executionInfo":{"status":"ok","timestamp":1619817305918,"user_tz":-120,"elapsed":3386,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"d19aeac7-9f35-4d34-e028-21f33eff17d6"},"source":["# \"stream_id\" na values in streams DF\n","df_streams.stream_id.isna().value_counts()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False    17695\n","True      1164\n","Name: stream_id, dtype: int64"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"7LFlJ4tvMKfE","executionInfo":{"status":"ok","timestamp":1619817305919,"user_tz":-120,"elapsed":3384,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}}},"source":["# dropping na's\n","df_streams.dropna(inplace=True)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTqlTJscNRAN","executionInfo":{"status":"ok","timestamp":1619817306226,"user_tz":-120,"elapsed":3687,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"1079e3c6-cfbc-456d-cac6-e124f7ffc6cf"},"source":["# new \"stream_id\" na values in streams DF \n","df_streams.stream_id.isna().sum()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"y6s8ScrpIdJT"},"source":["### QUESTION 4: \n","\n","**combine the data into a single data structure**\n","\n","For this example, the two most convenient structures for this task are Pandas dataframes and NumPy arrays.  At a minimum ensure that your structure accommodates the following.\n","\n","1. A column for `customer_id`\n","2. A column for `country`\n","3. A column for ```age``` that is created from ```DOB```\n","4. A column ```customer_name``` that is created from ```first_name``` and ```last_name```\n","5. A column to indicate churn called ```is_subscriber```\n","7. A column that indicates ```subscriber_type``` that comes from ```invoice_item```\n","6. A column to indicate the total ```num_streams```\n","\n","> HINT: For the subscriber type use the most frequent invoice_item_id and link it to the relevant invoice_item thanks to the INVOICE table in the database\n","\n","#### Resources\n","\n","* [Python's datetime library](https://docs.python.org/3/library/datetime.html)\n","* [NumPy's datetime data type](https://docs.scipy.org/doc/numpy/reference/arrays.datetime.html)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hfWygFuOGIT","executionInfo":{"status":"ok","timestamp":1619817306228,"user_tz":-120,"elapsed":3685,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"b9e72178-98d1-4150-fc63-d7a0b2ddf06b"},"source":["now = pd.Timestamp('now')\n","dob = pd.to_datetime(customer_df.DOB, infer_datetime_format=True)\n","dob = dob.where(dob < now, dob - np.timedelta64(100, 'Y'))\n","customer_age = (now - dob).astype('timedelta64[Y]')\n","customer_age"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0      22.0\n","1      32.0\n","2      23.0\n","3      22.0\n","4      23.0\n","       ... \n","995    56.0\n","996    24.0\n","997    26.0\n","998    41.0\n","999    21.0\n","Name: DOB, Length: 1000, dtype: float64"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"UFGcz09oIdJT","executionInfo":{"status":"ok","timestamp":1619817306228,"user_tz":-120,"elapsed":3682,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}}},"source":["## YOUR CODE HERE\n","# DataFrame Combined\n","df_comb = pd.DataFrame()\n","df_comb['customer_id'] = customer_df.customer_id\n","df_comb['customer_name'] = customer_df.first_name + ' ' + customer_df.last_name\n","df_comb['customer_age'] = customer_age.astype(int)\n","df_comb['country'] = customer_df.country_name\n","df_comb['is_subscriber'] = df_streams.groupby(\"customer_id\").max().subscription_stopped.values\n","df_comb['subscriber_type'] = df_streams.groupby(\"customer_id\").max().invoice_item_id.values.astype(int)\n","df_comb['num_streams'] = df_streams.groupby('customer_id').size().values"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"1a_HGCo_ULAC","executionInfo":{"status":"ok","timestamp":1619817306431,"user_tz":-120,"elapsed":3878,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"c1d92bc6-739b-4643-c0c1-579ca85ad1be"},"source":["df_comb"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>customer_id</th>\n","      <th>customer_name</th>\n","      <th>customer_age</th>\n","      <th>country</th>\n","      <th>is_subscriber</th>\n","      <th>subscriber_type</th>\n","      <th>num_streams</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Kasen Todd</td>\n","      <td>22</td>\n","      <td>united_states</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Ensley Garza</td>\n","      <td>32</td>\n","      <td>singapore</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Lillian Carey</td>\n","      <td>23</td>\n","      <td>united_states</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Beau Christensen</td>\n","      <td>22</td>\n","      <td>united_states</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Ernesto Gibson</td>\n","      <td>23</td>\n","      <td>singapore</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>996</td>\n","      <td>Peyton Enriquez</td>\n","      <td>56</td>\n","      <td>singapore</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>997</td>\n","      <td>Amina Manning</td>\n","      <td>24</td>\n","      <td>united_states</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>998</td>\n","      <td>Brooks Ventura</td>\n","      <td>26</td>\n","      <td>united_states</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>999</td>\n","      <td>Nayeli Mathis</td>\n","      <td>41</td>\n","      <td>united_states</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>1000</td>\n","      <td>Cole Solis</td>\n","      <td>21</td>\n","      <td>united_states</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>18</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 7 columns</p>\n","</div>"],"text/plain":["     customer_id     customer_name  ...  subscriber_type num_streams\n","0              1        Kasen Todd  ...                2          23\n","1              2      Ensley Garza  ...                3          12\n","2              3     Lillian Carey  ...                2          22\n","3              4  Beau Christensen  ...                1          19\n","4              5    Ernesto Gibson  ...                2          23\n","..           ...               ...  ...              ...         ...\n","995          996   Peyton Enriquez  ...                3          14\n","996          997     Amina Manning  ...                1          24\n","997          998    Brooks Ventura  ...                3          17\n","998          999     Nayeli Mathis  ...                3          16\n","999         1000        Cole Solis  ...                3          18\n","\n","[1000 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohvm-ob0iFqg","executionInfo":{"status":"ok","timestamp":1619817306433,"user_tz":-120,"elapsed":3874,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"3719def3-8119-4535-ed55-1dc6ce4548ef"},"source":["df_comb.info()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 1000 entries, 0 to 999\n","Data columns (total 7 columns):\n"," #   Column           Non-Null Count  Dtype \n","---  ------           --------------  ----- \n"," 0   customer_id      1000 non-null   int64 \n"," 1   customer_name    1000 non-null   object\n"," 2   customer_age     1000 non-null   int64 \n"," 3   country          1000 non-null   object\n"," 4   is_subscriber    1000 non-null   int64 \n"," 5   subscriber_type  1000 non-null   int64 \n"," 6   num_streams      1000 non-null   int64 \n","dtypes: int64(5), object(2)\n","memory usage: 62.5+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8tJUBro5IdJU"},"source":["## PART 3: Automating the process\n","\n","To ensure that you code can be used to automate this process.  First you will save you dataframe or numpy array as a CSV file.  \n","\n","### QUESTION 5:\n","\n","**Take the initial steps towards automation**\n","\n","1. Save your cleaned, combined data as a CSV file.\n","2. From the code above create a function or class that performs all of the steps given a database file and a streams CSV file.\n","3. Run the function in batches and write a check to ensure you got the same result that you did in the code above.\n","\n","There will be some logic involved to ensure that you do not write the same data twice to the target CSV file.\n","\n","Shown below is some code that will split your streams file into two batches. "]},{"cell_type":"markdown","metadata":{"id":"DPJ4j1j_mSeL"},"source":["```python\n","## code to split the streams csv into batches\n","df_all = pd.read_csv(os.path.join(data_dir,\"aavail-streams.csv\"))\n","customers_arr = df_all['customer_id'].unique()\n","# split the data set by customer ID.\n","customer_batches = customers_arr.reshape(2, int(customers_arr.shape[0]/2))\n","df_part1 = df_all[df_all['customer_id'].isin(customer_batches[0])]\n","df_part2 = df_all[df_all['customer_id'].isin(customer_batches[1])]\n","# save the batches as csv.\n","df_part1.to_csv(os.path.join(data_dir, \"aavail-streams-1.csv\"), index=False)\n","df_part2.to_csv(os.path.join(data_dir, \"aavail-streams-2.csv\"), index=False)\n","```"]},{"cell_type":"code","metadata":{"id":"ZDIaUpCUoNiH","executionInfo":{"status":"ok","timestamp":1619817306434,"user_tz":-120,"elapsed":3874,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}}},"source":["## YOUR CODE HERE\n","\n","# Saving the cleaned, combined data as a CSV file.\n","df_comb.to_csv(os.path.join(data_dir, \"aavail-subscribers.csv\"), index=False)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tUAxBBMkIdJV"},"source":["You will need to save your function as a .py file.  The following cell demonstrates how to do this from within a notebook. "]},{"cell_type":"code","metadata":{"id":"NCEDZy44IdJV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619817306435,"user_tz":-120,"elapsed":3871,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"54fd3d95-b64f-49e0-ddb9-a32c84ceb38c"},"source":["%%writefile aavail-data-ingestor.py\n","# The line above create a file called \"aavail-data-ingestor.py\" in the current working\n","# directory and write the reste of the cell in this file.\n","\n","import os\n","import sys\n","import getopt\n","import pandas as pd\n","import numpy as np\n","import sqlite3\n","\n","DATA_DIR = os.path.join(\"..\",\"data\")\n","\n","pass"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Overwriting aavail-data-ingestor.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nUrZwZxPIdJW"},"source":["You will also need to be able to pass the file names to your function without hardcoding them into the script itself.  This is an important step towards automation.  Here are the two libraries commonly used to accomplish this in Python.\n","\n","* [getopt](https://docs.python.org/3/library/getopt.html)\n","* [argparse](https://docs.python.org/3/library/argparse.html)"]},{"cell_type":"markdown","metadata":{"id":"ZmQ0KbGGIdJW"},"source":["You may run the script you just created from the commandline directly or from within this notebook using:\n","\n","```\n","!python aavail-data-ingestor.py aavail-customers.db aavail-streams.csv\n","\n","```"]},{"cell_type":"code","metadata":{"id":"kKJkekULIdJX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619817306435,"user_tz":-120,"elapsed":3866,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"f2dc5935-f8e6-41d7-bd8d-8ff6c366030a"},"source":["%%writefile aavail-data-ingestor.py\n","\n","\n","import os\n","import sys\n","import getopt\n","import pandas as pd\n","import numpy as np\n","import sqlite3\n","\n","DATA_DIR = os.path.join(\"..\",\"data\")\n","\n","\n","def connect_db(file_path):\n","    \"\"\"\n","    function to connection to aavail database\n","    INPUT : the file path of the data base\n","    OUTPUT : the sqlite connection to the database\n","    \"\"\"\n","    ## YOUR CODE HERE\n","    try:\n","      conn = sqlite3.connect(file_path)\n","      print(\"...successfully connected to db\\n\")\n","    except Error as e:\n","      print(\"...unsuccessful connection\\n\",e)\n","    \n","    return(conn)\n","\n","def ingest_db_data(conn):\n","    \"\"\"\n","    load and clean the db data\n","    INPUT : the sqlite connection to the database\n","    OUPUT : the customer dataframe\n","    \"\"\"\n","    ## YOUR CODE HERE\n","    customer_df = conn.execute(\n","        'SELECT customer_id, last_name, first_name, DOB, city, state, country_name, gender \\\n","        FROM CUSTOMER \\\n","        INNER JOIN COUNTRY ON CUSTOMER.country_id = COUNTRY.country_id;')\n","\n","    columns_name = ['customer_id', 'last_name', 'first_name', 'DOB', 'city', 'state', 'country_name', 'gender']\n","\n","    customer_df = pd.DataFrame(customer_df, columns=columns_name)\n","\n","    \n","    # Remove duplicates\n","    size_before = len(customer_df)\n","    customer_df.drop_duplicates(inplace=True)\n","    size_after = len(customer_df)\n","    print(\"... removed {} duplicate rows in customer data\".format(size_before-size_after))\n","    return customer_df\n","    \n","\n","def ingest_stream_data(file_path):\n","    \"\"\"\n","    load and clean the stream data\n","    INPUT : the file path of the stream csv\n","    OUTPUT : the streams dataframe and a mapping of the customers that churned\n","    \"\"\"\n","    ## YOUR CODE HERE\n","    df_streams = pd.read_csv(file_path)\n","    size_before = len(df_streams)\n","    df_streams = df_streams[~df_streams['stream_id'].isna()]\n","    size_after = len(df_streams)\n","    print(\"... removed {} missing stream ids\".format(size_before-size_after))\n","\n","    has_stopped = df_streams.groupby(\"customer_id\").apply(lambda x : True if x['subscription_stopped'].max() > 0 else False)\n","\n","    return df_streams, has_stopped\n","    \n","\n","def process_dataframes(customer_df, df_streams, has_stopped, conn):\n","    \"\"\"\n","    create the target dataset from the different data imported \n","    INPUT : the customer dataframe, the stream data frame, the map of churned customers and the connection to the database\n","    OUTPUT : the cleaned data set as described question 4\n","    \"\"\"\n","    ## YOUR CODE HERE\n","    df_comb = pd.DataFrame()\n","    df_comb['customer_id'] = customer_df.customer_id\n","    df_comb['customer_name'] = customer_df.first_name + ' ' + customer_df.last_name\n","\n","    # Create age column from DOB\n","    now = pd.Timestamp('now')\n","    dob = pd.to_datetime(customer_df.DOB, infer_datetime_format=True)\n","    dob = dob.where(dob < now, dob - np.timedelta64(100, 'Y'))\n","    customer_age = (now - dob).astype('timedelta64[Y]')\n","    customer_age\n","    df_comb['customer_age'] = customer_age.astype(int)\n","\n","    df_comb['country'] = customer_df.country_name\n","    df_comb['is_subscriber'] = df_streams.groupby(\"customer_id\").max().subscription_stopped.values\n","    df_comb['subscriber_type'] = df_streams.groupby(\"customer_id\").max().invoice_item_id.values.astype(int)\n","    df_comb['num_streams'] = df_streams.groupby('customer_id').size().values\n","\n","    return (df_comb)\n","    \n","    \n","def update_target(target_file, df_comb, overwrite=False):\n","    \"\"\"\n","    write the clean data in a target file located in the working directory.\n","    Overwrite the existing target file if overwrite is false, otherwise append the clean data to the target file\n","    INPUT : the name of the target file, the cleaned dataframe and an overwrite flag\n","    OUPUT : None\n","    \"\"\"\n","    ## YOUR CODE HERE\n","    if overwrite or not os.path.exists(target_file):\n","        df_comb.to_csv(target_file, index=False)   \n","    else:\n","        df_comb.to_csv(target_file, mode='a', header=False, index=False)\n","    \n","        \n","if __name__ == \"__main__\":\n","  \n","    ## collect and handle arguments with getopt or argparse\n","    ## YOUR CODE HERE\n","\n","    ## collect args\n","    arg_string = \"%s -d db_filepath -s streams_filepath\"%sys.argv[0]\n","    try:\n","        optlist, args = getopt.getopt(sys.argv[1:],'d:s:')\n","    except getopt.GetoptError:\n","        print(getopt.GetoptError)\n","        raise Exception(arg_string)\n","\n","    ## handle args\n","    streams_file = None\n","    db_file = None\n","    for o, a in optlist:\n","        if o == '-d':\n","            db_file = a\n","        if o == '-s':\n","            streams_file = a\n","    streams_file = os.path.join(DATA_DIR,streams_file)\n","    db_file = os.path.join(DATA_DIR,db_file)\n","    target_file = os.path.join(DATA_DIR, \"aavail-subscribers.csv\")\n","    \n","    ## make the connection to the database\n","    ## YOUR CODE HERE\n","    conn = connect_db(db_file)\n","\n","    ## ingest the data and transform it\n","    ## YOUR CODE HERE\n","    customer_df = ingest_db_data(conn)\n","    df_streams, df_churn = ingest_stream_data(streams_file)\n","    df_comb = process_dataframes(customer_df, df_streams, df_churn, conn)\n","    \n","    ## write the transformed data to a csv\n","    ## YOUR CODE HERE\n","    update_target(target_file, df_comb, overwrite=False)\n","    print(\"done\")\n","\n","    "],"execution_count":25,"outputs":[{"output_type":"stream","text":["Overwriting aavail-data-ingestor.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HWU5T5VGIdJX"},"source":["Run the script once for each batch that you created and then load both the original and batch versions back into the notebook to check that they are the same. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfrCJKxNhAfp","executionInfo":{"status":"ok","timestamp":1619817307885,"user_tz":-120,"elapsed":5312,"user":{"displayName":"Luis Alejandro Vallejo Mendez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghge5WPgueXxsed-jOdJ1B0kFpMUaz9jIjryoH2=s64","userId":"16228669718391012173"}},"outputId":"a6b96aaa-2276-46e0-f685-98bd0b2ba49c"},"source":["## YOUR CODE HERE\n","\n","!rm ../data/aavail-subscribers.csv\n","!python aavail-data-ingestor.py -d aavail-customers.db -s aavail-streams.csv\n","!wc -l ../data/aavail-subscribers.csv"],"execution_count":26,"outputs":[{"output_type":"stream","text":["...successfully connected to db\n","\n","... removed 7 duplicate rows in customer data\n","... removed 1164 missing stream ids\n","done\n","1001 ../data/aavail-subscribers.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I3s34jThIdJY"},"source":["### QUESTION 6:\n","\n","**How can you improve the process?**\n","\n","In paragraph form or using bullets write down some of the ways that you could improve this pipeline."]},{"cell_type":"markdown","metadata":{"id":"X0pCnq6yIdJY"},"source":["YOUR ANSWER HERE\n","\n","\n"]}]}